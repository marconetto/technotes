{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>I have a few code samples, walkthroughs, projects, experiments, links, and general notes that have been spread into multiple git repositories. I needed some more structure to see all this content myself, which can also be helpful to someone else. So I've decided to create this repo to serve as a hub for this content. Things here are not up-to-date, have no warranty, etc...</p> <p>Some topics here are:</p> <ul> <li>HPC</li> <li>Containers</li> <li>Analytics / ML</li> <li>Scripts / linux</li> <li>Distributed systems in general</li> <li>Projects</li> </ul>"},{"location":"ai/llm/","title":"LLM","text":"<p>An LLM (Large Language Model) is an AI system trained on larges amount of data so it can understand and generate human-like language. It predicts the next word in a sequence, and therefore it can:</p> <ul> <li>Answer questions</li> <li>Write code</li> <li>Translate text</li> <li>Summarize documents</li> <li>Hold conversations</li> </ul> <p>Some examples of models are: GPT-4, Claude, and Mistral.</p>"},{"location":"ai/llm/#hello-world-pre-trained-api-model","title":"Hello World: Pre-Trained / API Model","text":""},{"location":"ai/llm/#openai-cli","title":"OpenAI CLI","text":"<p>Installs the OpenAI Python client library; which are ysed to interact with OpenAI models (e.g. gpt-3.5, gpt-4, etc).</p> <pre><code>pip3 install openai\n</code></pre> <p>This is not enough to find the <code>openai</code> binary:</p> <pre><code>export PATH=\"$PATH:$(python3 -c 'import sysconfig; print(sysconfig.get_path(\"scripts\"))')\"\n</code></pre> <p>The following command should then work:</p> <pre><code>openai --help\n</code></pre> <p>Get the API key from: <code>https://platform.openai.com/account/api-keys</code></p> <p>Then execute:</p> <pre><code>openai api chat.completions.create -m gpt-3.5-turbo -g user \"Hello, world\"\n</code></pre>"},{"location":"ai/llm/#gemini-cli","title":"Gemini CLI","text":"<p>Gemini is a large language and multimodal model built by Google DeepMind.</p> <p>It can understand and generate: Text (like chat or code), images, audio, video, and structured data.</p> <p>Here are the steps to install and run the client that interacts with the model host in the cloud.</p> <pre><code>brew install gemini-cli\n</code></pre> <p>Starting gemini by selecting authentication method.</p> <pre><code>gemini\n</code></pre> <p>After this is done, one can ask questions there in an interactive session.</p> <p>One can also use <code>-p</code> (prompt).</p> <pre><code>gemini -p \"what was the most common programming language in the 90s\"\n</code></pre>"},{"location":"ai/llm/#ollama","title":"Ollama","text":"<p>Ollama, which statns for Ollama stands for (Omni-Layer Learning Language Acquisition Model), runs LLMs locally (like Llama 3, Mistral, Phi-4, etc.) with GPU/CPU acceleration; so need to use cloud to run queries to a model.</p> <pre><code>brew install ollama\nollama serve\nollama pull mistral\n</code></pre> <p>There are several ways to run/interact with the model.</p> <pre><code># interactive mode\nollama run mistral\n\n# query as parameter\nollama run mistral \"largest country in the planet?\"\n\n# or use here-document\nollama run mistral &lt;&lt;'EOF'\nlargest country in the planet?\nEOF\n\n# via http request\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\": \"largest country in the planet?\"\n}'\n</code></pre>"},{"location":"ai/llm/#openrounter","title":"OpenRounter","text":"<p>OpenRouter is a unified API gateway for multiple LLMs, including Claude, Mistral, and others. It is a bridge that lets one call different LLMs from one interface \u2014 via CLI, Python, or HTTP \u2014 without needing separate SDKs or accounts for each model.</p> <pre><code>brew install node\nnpm install -g @letuscode/openrouter-cli\nexport OPENROUTER_API_KEY=\"&lt;your key&gt;\"\nopenrouter test\n\n# select model\nopenrouter init\n\n# ask question\nopenrouter   ask \"largest country in the planet\"\n</code></pre>"},{"location":"ai/llm/#hugging-face","title":"Hugging Face","text":"<p>Hugging Face has a CLI to work with multiple LLMs.</p> <pre><code>pip3 install huggingface_hub\n\n# in case one wants to use the hf cli\n# export PATH=\"$PATH:$(python3 -c 'import sysconfig; print(sysconfig.get_path(\"scripts\"))')\"\n</code></pre> <p>Setup <code>HF_TOKEN</code>, save the following content as a python file and run it.</p> <pre><code>import os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"https://router.huggingface.co/v1\",\n    api_key=os.environ[\"HF_TOKEN\"],\n)\n\ncompletion = client.chat.completions.create(\n    model=\"openai/gpt-oss-120b:cerebras\",\n    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n)\n\nprint(completion.choices[0].message)\n</code></pre> <p>Or using CLI:</p> <pre><code>curl https://router.huggingface.co/v1/chat/completions \\\n  -H \"Authorization: Bearer $HF_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai/gpt-oss-120b:cerebras\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n  }' | jq -r '.choices[0].message.content'\n</code></pre>"},{"location":"containers/","title":"Containers","text":"<p>Links, codes, tutorials on container related technologies.</p>"},{"location":"containers/containerintro/","title":"Container Intro","text":"<p>Containers refer to a lightweight, portable, and self-sufficient unit of software that puts together code and all its dependencies so one can run an application/service.</p> <p>Virtual machine (VM)s virtualize the underlying hardware so that multiple operating system (OS) instances can run on that hardware. Each VM runs an OS and has access to virtualized resources representing the underlying hardware. On the other hand, a container virtualizes the underlying OS and causes the containerized app to perceive that it has the OS\u2014including CPU, memory, file storage, and network connections\u2014all to itself. Besides, containers share the host OS, so they do not need to boot an OS. So contenarized applications can start much faster.</p> <p>There are several container runtimes available, including Docker, Podman, containerd, Apptainer (Singularity).</p>"},{"location":"containers/containerintro/#quick-docker-example","title":"Quick docker example","text":"<p>Here we assume you have docker install on your machine (e.g. on macos: <code>brew install --cask docker</code> or install it by going to the docker website).</p> <p>Once installed you can run</p> <pre><code>$ docker run hello-world\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n</code></pre> <pre><code>$ docker images\nREPOSITORY    TAG       IMAGE ID       CREATED        SIZE\nhello-world   latest    74cc54e27dc4   4 months ago   10.1kB\n</code></pre> <p>You can create your own image. (files: folder) )</p> <p>Create <code>app.py</code> with this content:</p> <pre><code># app.py\nprint(\"Hello, world from Docker!\")\n</code></pre> <p>Create this <code>Dockerfile</code>:</p> Dockerfile<pre><code># Use official Python image\nFROM python:3.12-slim\n\n# Set working directory\nWORKDIR /app\n\n# Copy source code\nCOPY app.py .\n\n# Run the script\nCMD [\"python\", \"app.py\"]\n</code></pre> <p>Run:</p> <pre><code>$ docker build -t hello-python .\n$ docker run hello-python\nHello, world from Docker!\n</code></pre> <pre><code>$ docker images\nREPOSITORY     TAG       IMAGE ID       CREATED          SIZE\nhello-python   latest    e8cc71bba03c   45 seconds ago   124MB\nhello-world    latest    74cc54e27dc4   4 months ago     10.1kB\n</code></pre>"},{"location":"containers/containerintro/#quick-podman-example","title":"Quick podman example","text":"<p>Podman has basically the same syntax then docker. However, docker does not have a daemon as a requirement.</p> <p>Install it:</p> <p><pre><code>brew install podman\n</code></pre> Start the VM (required in macos; for linux you don't need that)</p> <pre><code>podman machine init\npodman machine start\n</code></pre> <p>Then you can do the same commands as described in the docker section above. For instance:</p> <pre><code>$ podman run hello-world\nResolved \"hello-world\" as an alias (/etc/containers/registries.conf.d/000-shortnames.conf)\nTrying to pull quay.io/podman/hello:latest...\nGetting image source signatures\nCopying blob sha256:81df7ff16254ed9756e27c8de9ceb02a9568228fccadbf080f41cc5eb5118a44\nCopying config sha256:5dd467fce50b56951185da365b5feee75409968cbab5767b9b59e325fb2ecbc0\nWriting manifest to image destination\n!... Hello Podman World ...!\n</code></pre> <p>You can check the podman VM running:</p> <pre><code>$ podman machine list\nNAME                     VM TYPE     CREATED        LAST UP            CPUS        MEMORY      DISK SIZE\npodman-machine-default*  applehv     4 minutes ago  Currently running  4           2GiB        100GiB\n</code></pre> <p>You can turn it off:</p> <pre><code>podman machine stop\n</code></pre>"},{"location":"containers/containerintro/#other-notes","title":"Other notes","text":"<p>If you want to get into a container image to explore file system for instance:</p> <pre><code>docker run -it --rm &lt;myimage&gt; bash\n</code></pre> <pre><code>-it: interactive terminal\n--rm: remove container on exit\n&lt;myimage&gt;: use the image your original container used\n</code></pre> <p>You can check the content of a file in the image for instance:</p> <pre><code>docker run -it --rm hello-python cat /app/app.py\n</code></pre> <p>If you want to get into a running container:</p> <pre><code>docker exec -it &lt;mycontainer&gt; bash\n</code></pre> <p>Or just check something there:</p> <pre><code>docker exec -it &lt;mycontainer&gt; ls /app\n</code></pre>"},{"location":"containers/containerintro/#linux","title":"Linux","text":"<p>In Linux, docker may fail because it says the daemon: \"docker: permission denied while trying to connect to the Docker daemon socket\"</p> <p>By default, Docker runs as a root-owned service. To manage Docker as a non-root user, your user needs to be part of the docker group. This allows you to run Docker commands without needing sudo. Add user to docker group:</p> <pre><code>sudo usermod -aG docker $USER\n</code></pre>"},{"location":"containers/minikubeintro/","title":"Minikube intro","text":"<p>Minikube is a tool that simplifies running a local Kubernetes cluster by launching a single-node cluster on a laptop/desktop/server using virtual machines, containers, or bare metal (depending on OS). It uses the same upstream Kubernetes binaries as production environments, making it ideal for development, testing, and learning Kubernetes without the overhead of managing a full-scale cluster. Minikube is not a separate version or subset of Kubernetes\u2014it doesn't modify Kubernetes core functionality or APIs. Instead, it is a setup and convenience layer around Kubernetes that streamlines installation, networking, and basic tooling for local use. It is not intended for production use, nor does it replicate the performance or complexity of multi-node, cloud-native Kubernetes clusters.</p>"},{"location":"containers/minikubeintro/#install-minikube-macos","title":"Install minikube (macos)","text":"<pre><code>brew install minikube\nbrew install kubectl\n</code></pre>"},{"location":"containers/minikubeintro/#start-minikube","title":"Start minikube","text":"<pre><code>minikube start --driver=docker\n</code></pre> <p>This launches a local Kubernetes cluster (in a docker container). This will download the VM image too in case it does not exist.</p>"},{"location":"containers/minikubeintro/#interact-with-the-cluster","title":"Interact with the cluster","text":"<p>Start minikube dashboard</p> <pre><code>minikube dashboard\n</code></pre> <p>See all pods in all namespaces (<code>get po</code> short for <code>get pods</code> and <code>-A</code> short for <code>--all-namespaces</code>).</p> <pre><code>$ kubectl get po -A\nNAMESPACE              NAME                                         READY   STATUS    RESTARTS        AGE\nkube-system            coredns-674b8bbfcf-sgpsl                     1/1     Running   0               5m16s\nkube-system            etcd-minikube                                1/1     Running   0               5m23s\nkube-system            kube-apiserver-minikube                      1/1     Running   0               5m22s\nkube-system            kube-controller-manager-minikube             1/1     Running   0               5m22s\nkube-system            kube-proxy-n55xw                             1/1     Running   0               5m17s\nkube-system            kube-scheduler-minikube                      1/1     Running   0               5m23s\nkube-system            storage-provisioner                          1/1     Running   1 (4m46s ago)   5m20s\nkubernetes-dashboard   dashboard-metrics-scraper-5d59dccf9b-724xl   1/1     Running   0               3m5s\nkubernetes-dashboard   kubernetes-dashboard-7779f9b69b-7mvvp        1/1     Running   0               3m5s\n</code></pre> <p></p>"},{"location":"containers/minikubeintro/#quick-description-of-the-pods","title":"Quick description of the pods","text":"<ul> <li> <p>coredns: Provides DNS resolution inside the cluster so services can find and communicate with each other.</p> </li> <li> <p>etcd: A distributed key-value store used to hold all cluster data and configuration.</p> </li> <li> <p>kube-apiserver: Serves as the front end of the Kubernetes control plane, exposing the Kubernetes API.</p> </li> <li> <p>kube-controller-manager: Runs controller processes that handle routine cluster tasks such as replication and node lifecycle management.</p> </li> <li> <p>kube-proxy: Maintains network rules on nodes, enabling network communication between pods and services.</p> </li> <li> <p>kube-scheduler: Assigns newly created pods to nodes based on resource availability and constraints.</p> </li> <li> <p>storage-provisioner: Automatically provisions persistent volumes for pods based on PersistentVolumeClaims (PVCs).</p> </li> <li> <p>dashboard-metrics-scraper: Collects resource usage metrics (CPU, memory) for display in the Kubernetes Dashboard.</p> </li> <li> <p>kubernetes-dashboard: A web-based UI for managing and monitoring Kubernetes resources.</p> </li> </ul>"},{"location":"containers/minikubeintro/#deploy-a-hello-world-app","title":"Deploy a hello world app","text":"<p>Create and expose deployment.</p> <pre><code>kubectl create deployment hello-minikube --image=kicbase/echo-server:1.0\nkubectl expose deployment hello-minikube --type=NodePort --port=8080\n</code></pre> <p>To see the service is running:</p> <pre><code>kubectl get services hello-minikube\n</code></pre> <p>To access the service by allowing minikube to launch a webserver:</p> <pre><code>minikube service hello-minikube\n</code></pre> <p>In macos you may get this error (in case you are using qemu driver):</p> <pre><code>Exiting due to MK_UNIMPLEMENTED: minikube service is not currently implemented with the builtin network on QEMU, try starting minikube with '--network=socket_vmnet'\n</code></pre> <p>One way to overcome this is:</p> <pre><code>kubectl port-forward service/hello-minikube 7080:8080\n</code></pre> <p>Then open the browser: <code>http://localhost:7080/</code></p> <p>Another is to install <code>socket_vmnet</code> (see instructions), then start the process to create minikube again, as it will detect the <code>socket_vmnet</code> installation.</p>"},{"location":"containers/minikubeintro/#delete-cluster","title":"Delete cluster","text":"<p>This will stop the VM running minikube and delete the cluster state.</p> <pre><code>minikube delete\n</code></pre>"},{"location":"containers/minikubeintro/#references","title":"References","text":"<ul> <li>Minikube</li> <li>Kubernetes 101</li> <li>Minikube macos tutorial</li> </ul>"},{"location":"hpc/","title":"HPC","text":"<p>Links, codes, tutorials on High Performance Computing (HPC).</p>"},{"location":"hpc/azurehpc/","title":"Azure adventures","text":"<p>Set of tutorials to work with Azure cloud/hpc technologies.</p> <p>Website: Azure Adventures</p>"},{"location":"hpc/hpcadvisor/","title":"HPC Advisor","text":"<p>Tool is to assist users in selecting a cluster configuration based on actual application executions. Underneath, the execution of the application is based on resources provisioned by Azure Batch; but the data collected can be used to make decisions for other environments including Azure CycleCloud, Azure VMSS, Azure VMs, AKS, etc.</p> <p>URL: HPCAdvisor</p>"},{"location":"hpc/hpcapps/","title":"HPC Apps","text":"<p>Repo containing installation and example of executions for some HPC applications including WRF, GROMACS, OpenFOAM, NAMD, and LAMMPS.</p> <p>URL: HPCApps repo</p>"},{"location":"hpc/mpitestcodes/","title":"Simple MPI test codes","text":"<p>Just some simple MPI code for tests.</p> <p>Source folder on GitHub: folder</p>"},{"location":"misc/ansible/","title":"Ansible intro","text":"<p>Ansible is an open-source automation tool for configuration management, application deployment, and orchestration. It uses YAML playbooks to describe the desired state of systems and communicates over SSH. It is agentless, which means there is no need to install anything on the target machines.</p>"},{"location":"misc/ansible/#why-ansible","title":"Why Ansible?","text":"<ul> <li>Agentless: No need to install extra software on managed machines;</li> <li>Idempotent: Tasks can be run repeatedly without unwanted side effects;</li> <li>Human-readable: Playbooks use YAML, easy to write and review;</li> <li>Scalable: Manage a handful of servers or thousands with the same commands.</li> </ul> <p>Some things we have inside a Dockerfile for instance are specific for docker. However, if you put the instructions in an ansible playbook, those can be executed outside docker, making this solution more reusable by different environments. If you are working with docker you could have also a playbook to have instructions that are targetted to the host and to the docker container.</p>"},{"location":"misc/ansible/#alternatives","title":"Alternatives","text":"<p>If Ansible is not available, one could use:</p> <ul> <li>Shell scripts with <code>ssh</code> and <code>scp</code> for manual automation;</li> <li>Configuration management tools like Puppet or  Chef (these often require agents on target mmachines);</li> <li>Cloud-native tools such as AWS CloudFormation, Terraform, or Azure Resource Manager (for infrastructure provisioning).</li> </ul>"},{"location":"misc/ansible/#major-components","title":"Major components","text":"<ul> <li>Inventory: A list of machines (hosts) Ansible manages.</li> <li>Modules: Small, reusable programs that perform one action (install a package, copy a file, manage a service, etc.).</li> <li>Tasks: Single steps executed on target hosts; each task calls a module with parameters.</li> <li>Plays: A mapping that says \u201crun these tasks on these hosts\u201d.</li> <li>Playbooks: YAML files containing one or more plays. They describe how and which order, at what time and where, and what modules should be executed. It orchestrates the modules' execution.</li> <li>Handlers: Special tasks executed only when notified by other tasks (useful for actions like restarting a service after config change).</li> <li>Roles: Redistributable units of organization that allow users to share automation code easier.</li> <li>Variables: Key\u2013value pairs to parametrize playbooks, templates, and roles.</li> </ul>"},{"location":"misc/ansible/#simple-test-vm","title":"Simple test VM","text":"<p>Simple example, for instance, once you provision a cloud VM (e.g. almalinux vm).</p> <pre><code>sudo dnf -y install epel-release\nsudo dnf -y install ansible\nansible --version\n</code></pre> <p>Create inventory file (<code>inventory</code>):</p> <pre><code>[local]\nlocalhost ansible_connection=local\n</code></pre> <p>Create playbook file (<code>simple.yml</code>). This is a playbook with a single play and three tasks.</p> Simple<pre><code>- name: Simple Ansible test on AlmaLinux\n  hosts: local\n  connection: local\n  tasks:\n    - name: Gather system facts\n      setup:\n    - name: Create a test file\n      copy:\n        dest: /tmp/ansible_hello.txt\n        content: \"Hello from Ansible at {{ ansible_date_time.iso8601 }}\\n\"\n    - name: Show hostname\n      debug:\n        msg: \"This VM's hostname is {{ ansible_hostname }}\"\n</code></pre> <p>Run the line below and the file in <code>/tmp/</code> should be created.</p> <pre><code>ansible-playbook -i inventory simple.yml\n</code></pre> <p>If one wants to use multiple machines, replace 'hosts: local' with 'hosts: all' in the <code>simple.yml</code> and update the inventory with the list of ip addresses. Also remove <code>connection: local</code>.</p>"},{"location":"misc/ansible/#further-notes","title":"Further notes","text":"<p>List available modules:</p> <pre><code>ansible-doc -l\n</code></pre> <p>More details on a particular module (e.g. <code>copy</code>):</p> <pre><code>ansible-doc copy\n</code></pre> <p>Can also find modules docs online at: WEBSITE</p> <p>Test connections:</p> <pre><code>ansible all -m ping -i inventory\n</code></pre> <p>Show inventory:</p> <pre><code>ansible-inventory --list -y -i inventory\n</code></pre> <p>Append extra verbose mode using <code>-vvv</code>:</p> <pre><code>ansible-playbook -i inventory simple.yml -vvv\n</code></pre>"},{"location":"misc/blender/","title":"Blender","text":"<p>Blender 4.5 (video list): overview for beginners (UI, shortcuts, object transformation, animation, texture, light, etc..) HERE</p> <p>Simple: \"move the cube\" HERE</p> <p>Simple: \"move camera\" HERE</p> <p>Simple: \"move camera\" HERE</p> <p>Simple: \"move camera around\" HERE</p>"},{"location":"misc/grafana/","title":"Grafana","text":"<p>Grafana is a data visualization and monitoring tool. It connects to various data sources (like Prometheus, InfluxDB, MySQL, etc.) and allows one to create dashboards, graphs, and alerts to monitor metrics in real time.</p> <p>Here is a quick example with grafana + prometheus + node exporter on localhost (macos).</p> Component Port Purpose Node Exporter 9100 Exposes system metrics (CPU, memory, disk, etc.) Prometheus 9090 Collects metrics and stores them Grafana 3000 Visualizes metrics in dashboards <p>Assuming Node Exporter and Prometheus are already running.</p> <pre><code>brew install grafana\nbrew services start grafana\n</code></pre> <p>Open browser(admin/admin): <code>http://localhost:3000/login</code></p> <p>Go to Settings \u2192 Data Sources \u2192 Add data source \u2192 Prometheus.  Then add <code>http://localhost:9090</code>.</p> <p>To add node exporter; go to Dashboards, then import the dashboard using this ID: <code>1860</code>. This will import a pre-built dashboard for node exporter.</p> <p>To stress cpu and see changes in the dashboard:</p> <pre><code>brew install stress\nstress --cpu 4 --timeout 60\n</code></pre> <p>Alternatively, one can import dashboards:</p> cpu<pre><code>{\n  \"id\": null,\n  \"title\": \"CPU Utilization\",\n  \"tags\": [\n    \"example\"\n  ],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"5s\",\n  \"panels\": [\n    {\n      \"type\": \"timeseries\",\n      \"title\": \"CPU Utilization (%)\",\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"expr\": \"100 * (1 - avg(rate(node_cpu_seconds_total{mode=\\\"idle\\\"}[2m])))\",\n          \"legendFormat\": \"CPU Utilization\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"x\": 0,\n        \"y\": 0,\n        \"w\": 24,\n        \"h\": 9\n      }\n    }\n  ]\n}\n</code></pre> cpuanddisk<pre><code>{\n  \"id\": null,\n  \"title\": \"CPU and Disk Usage\",\n  \"tags\": [\n    \"example\"\n  ],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"5s\",\n  \"panels\": [\n    {\n      \"type\": \"timeseries\",\n      \"title\": \"CPU Utilization (%)\",\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"expr\": \"100 * (1 - avg(rate(node_cpu_seconds_total{mode=\\\"idle\\\"}[2m])))\",\n          \"legendFormat\": \"CPU Utilization\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"x\": 0,\n        \"y\": 0,\n        \"w\": 24,\n        \"h\": 9\n      }\n    },\n    {\n      \"type\": \"timeseries\",\n      \"title\": \"Disk Usage (%)\",\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"expr\": \"100 - (100 * (node_filesystem_avail_bytes{fstype!=\\\"tmpfs\\\",mountpoint!=\\\"/boot\\\"} / node_filesystem_size_bytes{fstype!=\\\"tmpfs\\\",mountpoint!=\\\"/boot\\\"}))\",\n          \"legendFormat\": \"{{mountpoint}}\",\n          \"refId\": \"B\"\n        }\n      ],\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"x\": 0,\n        \"y\": 9,\n        \"w\": 24,\n        \"h\": 9\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"misc/mlperf/","title":"MLPerf","text":"<p>MLPerf is an open, industry-standard benchmark suite for measuring machine learning performance. It\u2019s maintained by the MLCommons consortium (which includes NVIDIA, Google, Intel, and others). MLPerf is primarily categorized by two major types of ML processes: training and inference. There are also specialized benchmarks for specific hardware and workloads. Original documents describing MLPerf can be found here: MLPerf Training and MLPerf Inference. Those are interesting to understand some of the key motivations behind these benchmarks, including the uniqueness of ML/DL workloads w.r.t. benchmarking.</p> <p>MLPerf evaluates systems on standardized ML workloads across multiple domains, including:</p> <ul> <li>Image classification (e.g., ResNet-50)</li> <li>Object detection (e.g., SSD, Mask R-CNN)</li> <li>Language modeling (e.g., BERT)</li> <li>Recommendation systems (e.g., DLRM)</li> <li>Speech recognition (e.g., RNN-T)</li> </ul> <p>Full list for training can be found here and inference can be found here.</p> <p>One can use MLPerf results to: (i) Compare hardware (e.g., GPU vs CPU); (ii) Tune software stacks (CUDA, PyTorch, TensorFlow); and (iii) Validate scaling behavior or deployment efficiency.</p>"},{"location":"misc/mlperf/#testing-on-a-single-vm","title":"Testing on a single VM","text":""},{"location":"misc/mlperf/#training","title":"Training","text":"<p>Example: Single Stage Detector.</p> <p>In this example we assume we have a VM in azure with SKU <code>Standard_NC40ads_H100_v5</code> and image <code>almalinux:almalinux-hpc:8_10-hpc-gen2:latest</code>. Once machine is provisioned, check if gpus+cuda is detected via: <code>nvidia-smi</code>.</p> prepmlperf<pre><code>#!/bin/bash\n# script to do all setup, data download, and training\n# based on https://github.com/mlcommons/training/tree/master/single_stage_detector\n\nexport DATADIR=\"/datadrive\"\nexport MYDATA=\"$DATADIR/mydata\"\nsudo groupadd docker\nsudo usermod -aG docker $USER\nnewgrp docker\ndocker run hello-world\n\nsudo jq '. + {\"data-root\": \"/mnt/docker\"}' /etc/docker/daemon.json | sudo tee /etc/docker/daemon.json.tmp &gt;/dev/null &amp;&amp; sudo mv /etc/docker/daemon.json.tmp /etc/docker/daemon.json &amp;&amp; sudo systemctl restart docker\ndocker info | grep 'Docker Root Dir'\n\ncd $DATADIR\ngit clone https://github.com/mlcommons/training.git\n\n# alternative for faster install (with possible side effects;e.g install on other python versions)\n# pip3 install --prefer-binary opencv-python-headless\n# sed -i.bak -E 's/load_zoo_dataset\\(\\s*name=\"([^\"]+)\"\\s*,/load_zoo_dataset(\"\\1\",/' fiftyone_openimages.py\n\npip3 install fiftyone\npython -m pip show fiftyone\nsudo ln -s $(which python3) /usr/local/bin/python\ncd $DATADIR/training/single_stage_detector/scripts\n\necho \"Downloading dataset... this will take several minutes\"\n./download_openimages_mlperf.sh -d $MYDATA\n\n# prepare docker\ncd $DATADIR/training/single_stage_detector/\ndocker build -t mlperf/single_stage_detector .\ndocker run --rm -it --gpus=all --ipc=host -v $MYDATA:/datasets/open-images-v6-mlperf mlperf/single_stage_detector bash\n\n# inside the container:\n# apt-get update ; apt-get install vim -y\n# sed -i '0,/\\${SLURM_LOCALID-}/s//${SLURM_LOCALID:-0}/' run_and_time.sh\n# update config_DGXA100_001x08x032.sh\n# DGXNGPU=1, DGXSOCKETCORES=20, DGXNSOCKET=1, DGXHT=2\n# source config_DGXA100_001x08x032.sh\n# -----------------------------------------------------------\n# conda create -n torch212 python=3.10\n# conda init bash\n# source ~/.bashrc\n# conda activate torch212\n# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n#\n# fix: coco_eval.py\n# # Old line:\n# import torch._six\n# #Replace with\n# import collections.abc as container_abcs\n#\n# pip3 install mlperf_logging\n# pip3 install pycocotools\n# ./run_and_time.sh\n</code></pre> <p>Once run script is started, output should be:</p> <pre><code>Epoch: [0]  [    0/36571]  eta: 1 day, 20:19:44  lr: 0.000000  loss: 2.2699 (2.2699)  classification: 1.5590 (1.5590)  bbox_reg\nression: 0.7109 (0.7109)  time: 4.3637  data: 2.2989  max mem: 51676\nEpoch: [0]  [   20/36571]  eta: 6:17:02  lr: 0.000000  loss: 2.1944 (2.2521)  classification: 1.4886 (1.5371)  bbox_regression:\n 0.7036 (0.7150)  time: 0.4317  data: 0.0003  max mem: 52125\nEpoch: [0]  [   40/36571]  eta: 5:20:59  lr: 0.000000  loss: 2.1934 (2.2440)  classification: 1.4949 (1.5292)  bbox_regression:\n 0.6956 (0.7148)  time: 0.4309  data: 0.0003  max mem: 52125\nEpoch: [0]  [   60/36571]  eta: 5:03:05  lr: 0.000000  loss: 2.2322 (2.2630)  classification: 1.5102 (1.5478)  bbox_regression:\n 0.7024 (0.7151)  time: 0.4384  data: 0.0004  max mem: 52125\n.\n.\n.\n</code></pre>"},{"location":"misc/mlperf/#multiple-vms","title":"Multiple VMs","text":"<p>Similar to single VM but a cluster needs to be provision and parameters related to SLURM need to be adjusted. See details HERE.</p>"},{"location":"misc/prometheus/","title":"Prometheus","text":"<p>Prometheus is an open-source monitoring and alerting system. It collects metrics from applications and systems, stores them in a time-series database, and lets one query or visualize the data easily.</p>"},{"location":"misc/prometheus/#test-locally-macos","title":"Test locally (macos)","text":"<pre><code>brew install prometheus\nbrew install node_exporter\n</code></pre> <p>Run node exporter and test it:</p> <pre><code>node_exporter\ncurl http://localhost:9100/metrics\n</code></pre> <p>Create <code>prometheus.yml</code>:</p> <pre><code>global:\n  scrape_interval: 5s  # how often to collect metrics\n\nscrape_configs:\n  - job_name: \"node\"\n    static_configs:\n      - targets: [\"localhost:9100\"]\n</code></pre> <p>Run prometheus:</p> <pre><code>prometheus --config.file=prometheus.yml\n</code></pre> <p>Open browser: <code>localhost:9090/query</code></p> <p>In the expression enter this to see the overal system utilization)</p> <pre><code>100 * (1 - avg(rate(node_cpu_seconds_total{mode=\"idle\"}[2m])))\n</code></pre> <p>One can also go to the settings menu to change to local time zone, as default is UTC.</p>"},{"location":"misc/prometheus/#refereces","title":"Refereces","text":"<ul> <li>https://prometheus.io/docs/prometheus/latest/getting_started/</li> </ul>"}]}