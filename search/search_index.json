{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p>I have a few code samples, walkthroughs, projects, experiments, links, and general notes that have been spread into multiple git repositories. I needed some more structure to see all this content myself, which can also be helpful to someone else. So I've decided to create this repo to serve as a hub for this content. Things here are not up-to-date, have no warranty, etc...</p> <p>Some topics here are:</p> <ul> <li>HPC</li> <li>Containers</li> <li>Analytics / ML</li> <li>Scripts / linux</li> <li>Distributed systems in general</li> <li>Projects</li> </ul>"},{"location":"ai/llm/","title":"LLM","text":"<p>An LLM (Large Language Model) is an AI system trained on larges amount of data so it can understand and generate human-like language. It predicts the next word in a sequence, and therefore it can:</p> <ul> <li>Answer questions</li> <li>Write code</li> <li>Translate text</li> <li>Summarize documents</li> <li>Hold conversations</li> </ul> <p>Some examples of models are: GPT-4, Claude, and Mistral.</p>"},{"location":"ai/llm/#hello-world-pre-trained-api-model","title":"Hello World: Pre-Trained / API Model","text":""},{"location":"ai/llm/#openai-cli","title":"OpenAI CLI","text":"<p>Installs the OpenAI Python client library; which are ysed to interact with OpenAI models (e.g. gpt-3.5, gpt-4, etc).</p> <pre><code>pip3 install openai\n</code></pre> <p>This is not enough to find the <code>openai</code> binary:</p> <pre><code>export PATH=\"$PATH:$(python3 -c 'import sysconfig; print(sysconfig.get_path(\"scripts\"))')\"\n</code></pre> <p>The following command should then work:</p> <pre><code>openai --help\n</code></pre> <p>Get the API key from: <code>https://platform.openai.com/account/api-keys</code></p> <p>Then execute:</p> <pre><code>openai api chat.completions.create -m gpt-3.5-turbo -g user \"Hello, world\"\n</code></pre>"},{"location":"ai/llm/#gemini-cli","title":"Gemini CLI","text":"<p>Gemini is a large language and multimodal model built by Google DeepMind.</p> <p>It can understand and generate: Text (like chat or code), images, audio, video, and structured data.</p> <p>Here are the steps to install and run the client that interacts with the model host in the cloud.</p> <pre><code>brew install gemini-cli\n</code></pre> <p>Starting gemini by selecting authentication method.</p> <pre><code>gemini\n</code></pre> <p>After this is done, one can ask questions there in an interactive session.</p> <p>One can also use <code>-p</code> (prompt).</p> <pre><code>gemini -p \"what was the most common programming language in the 90s\"\n</code></pre>"},{"location":"ai/llm/#ollama","title":"Ollama","text":"<p>Ollama, which statns for Ollama stands for (Omni-Layer Learning Language Acquisition Model), runs LLMs locally (like Llama 3, Mistral, Phi-4, etc.) with GPU/CPU acceleration; so need to use cloud to run queries to a model.</p> <pre><code>brew install ollama\nollama serve\nollama pull mistral\n</code></pre> <p>There are several ways to run/interact with the model.</p> <pre><code># interactive mode\nollama run mistral\n\n# query as parameter\nollama run mistral \"largest country in the planet?\"\n\n# or use here-document\nollama run mistral &lt;&lt;'EOF'\nlargest country in the planet?\nEOF\n\n# via http request\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\": \"largest country in the planet?\"\n}'\n</code></pre>"},{"location":"ai/llm/#openrounter","title":"OpenRounter","text":"<p>OpenRouter is a unified API gateway for multiple LLMs, including Claude, Mistral, and others. It is a bridge that lets one call different LLMs from one interface \u2014 via CLI, Python, or HTTP \u2014 without needing separate SDKs or accounts for each model.</p> <pre><code>brew install node\nnpm install -g @letuscode/openrouter-cli\nexport OPENROUTER_API_KEY=\"&lt;your key&gt;\"\nopenrouter test\n\n# select model\nopenrouter init\n\n# ask question\nopenrouter   ask \"largest country in the planet\"\n</code></pre>"},{"location":"ai/llm/#hugging-face","title":"Hugging Face","text":"<p>Hugging Face has a CLI to work with multiple LLMs.</p> <pre><code>pip3 install huggingface_hub\n\n# in case one wants to use the hf cli\n# export PATH=\"$PATH:$(python3 -c 'import sysconfig; print(sysconfig.get_path(\"scripts\"))')\"\n</code></pre> <p>Setup <code>HF_TOKEN</code>, save the following content as a python file and run it.</p> <pre><code>import os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"https://router.huggingface.co/v1\",\n    api_key=os.environ[\"HF_TOKEN\"],\n)\n\ncompletion = client.chat.completions.create(\n    model=\"openai/gpt-oss-120b:cerebras\",\n    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n)\n\nprint(completion.choices[0].message)\n</code></pre> <p>Or using CLI:</p> <pre><code>curl https://router.huggingface.co/v1/chat/completions \\\n  -H \"Authorization: Bearer $HF_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai/gpt-oss-120b:cerebras\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n  }' | jq -r '.choices[0].message.content'\n</code></pre>"},{"location":"containers/","title":"Containers","text":"<p>Links, codes, tutorials on container related technologies.</p>"},{"location":"containers/containerintro/","title":"Container Intro","text":"<p>Containers refer to a lightweight, portable, and self-sufficient unit of software that puts together code and all its dependencies so one can run an application/service.</p> <p>Virtual machine (VM)s virtualize the underlying hardware so that multiple operating system (OS) instances can run on that hardware. Each VM runs an OS and has access to virtualized resources representing the underlying hardware. On the other hand, a container virtualizes the underlying OS and causes the containerized app to perceive that it has the OS\u2014including CPU, memory, file storage, and network connections\u2014all to itself. Besides, containers share the host OS, so they do not need to boot an OS. So contenarized applications can start much faster.</p> <p>There are several container runtimes available, including Docker, Podman, containerd, Apptainer (Singularity).</p>"},{"location":"containers/containerintro/#quick-docker-example","title":"Quick docker example","text":"<p>Here we assume you have docker install on your machine (e.g. on macos: <code>brew install --cask docker</code> or install it by going to the docker website).</p> <p>Once installed you can run</p> <pre><code>$ docker run hello-world\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n</code></pre> <pre><code>$ docker images\nREPOSITORY    TAG       IMAGE ID       CREATED        SIZE\nhello-world   latest    74cc54e27dc4   4 months ago   10.1kB\n</code></pre> <p>You can create your own image. (files: folder) )</p> <p>Create <code>app.py</code> with this content:</p> <pre><code># app.py\nprint(\"Hello, world from Docker!\")\n</code></pre> <p>Create this <code>Dockerfile</code>:</p> Dockerfile<pre><code># Use official Python image\nFROM python:3.12-slim\n\n# Set working directory\nWORKDIR /app\n\n# Copy source code\nCOPY app.py .\n\n# Run the script\nCMD [\"python\", \"app.py\"]\n</code></pre> <p>Run:</p> <pre><code>$ docker build -t hello-python .\n$ docker run hello-python\nHello, world from Docker!\n</code></pre> <pre><code>$ docker images\nREPOSITORY     TAG       IMAGE ID       CREATED          SIZE\nhello-python   latest    e8cc71bba03c   45 seconds ago   124MB\nhello-world    latest    74cc54e27dc4   4 months ago     10.1kB\n</code></pre>"},{"location":"containers/containerintro/#quick-podman-example","title":"Quick podman example","text":"<p>Podman has basically the same syntax then docker. However, docker does not have a daemon as a requirement.</p> <p>Install it:</p> <p><pre><code>brew install podman\n</code></pre> Start the VM (required in macos; for linux you don't need that)</p> <pre><code>podman machine init\npodman machine start\n</code></pre> <p>Then you can do the same commands as described in the docker section above. For instance:</p> <pre><code>$ podman run hello-world\nResolved \"hello-world\" as an alias (/etc/containers/registries.conf.d/000-shortnames.conf)\nTrying to pull quay.io/podman/hello:latest...\nGetting image source signatures\nCopying blob sha256:81df7ff16254ed9756e27c8de9ceb02a9568228fccadbf080f41cc5eb5118a44\nCopying config sha256:5dd467fce50b56951185da365b5feee75409968cbab5767b9b59e325fb2ecbc0\nWriting manifest to image destination\n!... Hello Podman World ...!\n</code></pre> <p>You can check the podman VM running:</p> <pre><code>$ podman machine list\nNAME                     VM TYPE     CREATED        LAST UP            CPUS        MEMORY      DISK SIZE\npodman-machine-default*  applehv     4 minutes ago  Currently running  4           2GiB        100GiB\n</code></pre> <p>You can turn it off:</p> <pre><code>podman machine stop\n</code></pre>"},{"location":"containers/containerintro/#other-notes","title":"Other notes","text":"<p>If you want to get into a container image to explore file system for instance:</p> <pre><code>docker run -it --rm &lt;myimage&gt; bash\n</code></pre> <pre><code>-it: interactive terminal\n--rm: remove container on exit\n&lt;myimage&gt;: use the image your original container used\n</code></pre> <p>You can check the content of a file in the image for instance:</p> <pre><code>docker run -it --rm hello-python cat /app/app.py\n</code></pre> <p>If you want to get into a running container:</p> <pre><code>docker exec -it &lt;mycontainer&gt; bash\n</code></pre> <p>Or just check something there:</p> <pre><code>docker exec -it &lt;mycontainer&gt; ls /app\n</code></pre>"},{"location":"containers/containerintro/#linux","title":"Linux","text":"<p>In Linux, docker may fail because it says the daemon: \"docker: permission denied while trying to connect to the Docker daemon socket\"</p> <p>By default, Docker runs as a root-owned service. To manage Docker as a non-root user, your user needs to be part of the docker group. This allows you to run Docker commands without needing sudo. Add user to docker group:</p> <pre><code>sudo usermod -aG docker $USER\n</code></pre>"},{"location":"containers/containervocabulary/","title":"Container Vocabulary","text":"<ul> <li>Physical Machine \u2013 real hardware server.</li> <li>VM \u2013 virtual machine running its own OS.</li> <li>Node \u2013 a VM or machine that runs Kubernetes components or workloads. A node can run multiple pods.</li> <li>Pod \u2013 smallest deployable unit; one or more containers. K8s manages pods not containers</li> <li>Cluster \u2013 group of nodes managed as one Kubernetes system.</li> <li>Control Plane Node \u2013 node running API server, scheduler, controllers.</li> <li>Worker Node \u2013 node that runs application pods.</li> <li>Container \u2013 lightweight isolated environment for applications.</li> <li>Container Runtime \u2013 engine that runs containers (containerd, CRI-O).</li> <li>Deployment \u2013 manages desired number of pod replicas. It leverages ReplicaSet,   plus it supports Rolling updates (zero-downtime updates), rollbacks pausing/resuming updates, and revision history.</li> <li>ReplicaSet \u2013 maintains a stable set of identical pods.</li> <li>DaemonSet \u2013 runs exactly one pod per node.</li> <li>Job \u2013 runs a finite/one-time workload.</li> <li>JobSet - single object that defines several Jobs that must be treated as one workload. Supports: Multi-role distributed jobs, gang scheduling, and topology-aware scheduling.</li> <li>CronJob \u2013 runs jobs on a schedule.</li> <li>Service \u2013 stable networking endpoint to reach pods.</li> <li>ClusterIP \u2013 internal-only service endpoint.</li> <li>NodePort \u2013 exposes service on each node\u2019s port.</li> <li>LoadBalancer \u2013 exposes service externally via cloud LB.</li> <li>Ingress \u2013 HTTP/HTTPS routing into services.</li> <li>CNI \u2013 plugin that provides pod networking.</li> <li>Volume \u2013 storage attached to a pod.</li> <li>PersistentVolume \u2013 actual physical/logical storage resource.</li> <li>PersistentVolumeClaim \u2013 request for persistent storage.</li> <li>ConfigMap \u2013 object used to store (non-sensitive) configuration data (plain text) that applications need, separate from container images.</li> <li>Secret \u2013 sensitive configuration data.</li> <li>API Server \u2013 entry point for all Kubernetes commands.</li> <li>Scheduler \u2013 picks which node a pod runs on.</li> <li>Controller Manager \u2013 ensures desired state is maintained.</li> <li>etcd \u2013 key-value store for cluster state. Stores all configuration, desired state, and current state for every resource.</li> <li>Namespace \u2013 logical grouping of resources.</li> <li>Label \u2013 key-value metadata for selecting resources (e.g. pods, nodes, jobs, namespace, etc). Can be used to group, view, and operate on a subset of objects.</li> <li>Node Selector \u2013 restricts pods to certain nodes.</li> <li>Affinity \u2013 rules for preferred node placement.</li> <li>Taint \u2013 marks a node to repel pods.</li> <li>Toleration \u2013 allows pod to run on tainted nodes.</li> <li>Probes - health checks performed by the Kubelet on containers to monitor their state. Types: liveness probes (restart a container if it's unhealthy), readiness probes (ensure a container is ready to serve traffic before it receives any), and startup probes (for slow-starting containers to give them time to initialize before readiness and liveness checks begin).</li> <li>kubeclt - CLI for k8s clusters. Communicates with API server</li> <li>kubelet - node agent that runs on every node and makes sure the containers and pods scheduled to that node are running correctly. kubelet receives a pod spec from the control plane and instructs the container runtime to create the containers that make up that pod. The pod is created first (as an object in the Kubernetes API), and then the containers inside that pod are created on a node.</li> </ul>"},{"location":"containers/k8s/","title":"K8s","text":"<p>Kubernetes (k8s) is an open-source platform for automating deployment, scaling, and management of containerized applications. It was originally developed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF).</p> <p>Kubernetes is designed to manage clusters of machines (physical or virtual) and orchestrate containers such as Docker, ensuring applications run reliably and efficiently.</p> <p>Oview of operation:</p> <ul> <li>User defines desired state in YAML/JSON (e.g., 3 replicas of a web app).</li> <li>YAML is submitted to the API server.</li> <li>Scheduler decides which nodes will run the pods.</li> <li>kubelet on each node ensures the pod is running correctly.</li> <li>Controllers monitor the cluster and make corrections if the actual state deviates from desired state.</li> <li>Services provide networking and load balancing for pods.</li> </ul>"},{"location":"containers/k8s/#goals","title":"Goals","text":"<p>Kubernetes was designed with several goals in mind:</p> <ol> <li> <p>Automated Deployment and Scaling</p> <ul> <li>Automatically deploy containerized applications across nodes.</li> <li>Scale applications up or down based on demand.</li> </ul> </li> <li> <p>Self-Healing</p> <ul> <li>Automatically restarts failed containers.</li> <li>Replaces or reschedules containers if nodes die.</li> <li>Kills containers that don't respond to health checks.</li> </ul> </li> <li> <p>Service Discovery and Load Balancing</p> <ul> <li>Automatically exposes containers using DNS names or IP addresses.</li> <li>Distributes network traffic evenly across containers.</li> </ul> </li> <li> <p>Infrastructure Abstraction</p> <ul> <li>Applications run the same way whether nodes are on-premises, in a cloud, or hybrid.</li> </ul> </li> <li> <p>Declarative Configuration and Automation</p> <ul> <li>Users describe the desired state of the system (e.g., \"3 replicas of my app\") in YAML/JSON.</li> <li>Kubernetes automatically ensures the system matches the desired state.</li> </ul> </li> </ol>"},{"location":"containers/k8s/#components","title":"Components","text":"<p>A. Control Plane Components (master-level)</p> <p>These manage the cluster and ensure the desired state.</p> <ul> <li>kube-apiserver: Exposes the Kubernetes API. All commands and external interactions go through this.</li> <li>etcd: Distributed key-value store for cluster state and configuration.</li> <li>kube-scheduler: Assigns pods to nodes based on resource requirements, policies, and constraints.</li> <li>kube-controller-manager: Runs controllers that handle routine tasks like replicating pods or managing node health.</li> <li>cloud-controller-manager: Interfaces with cloud providers to manage resources like load balancers, storage, or networking.</li> </ul> <p>B. Node Components (worker-level)</p> <p>These run the actual workloads (containers) and report back to the control plane.</p> <ul> <li>kubelet: Agent that runs on each node, ensures containers are running as described in PodSpecs.</li> <li>kube-proxy: Handles networking for pods, including service IPs and load balancing.</li> <li>Container Runtime: Software that runs containers (e.g., containerd, Docker, CRI-O).</li> </ul> <p>C. Key Kubernetes Objects</p> <p>These are abstractions for managing containers:</p> <ul> <li>Pod: Smallest deployable unit; a group of one or more containers sharing resources.</li> <li>Service: Exposes a set of pods to network traffic with a stable IP and DNS name.</li> <li>Deployment: Declaratively manages a set of pods, handling scaling, rolling updates, and self-healing.</li> <li>ConfigMap / Secret: Stores configuration data or sensitive information for pods.</li> <li>Namespace: Provides logical isolation of resources in a cluster.</li> <li>Volume / Persistent Volume: Handles storage that persists beyond container lifetimes.</li> </ul>"},{"location":"containers/k8s/#example-k8s-on-a-single-vm","title":"Example k8s on a single VM","text":"<p>Provision a single VM (example via Azure services). In this example we have an almalinux VM being created. Once the VM is created, the following steps are used to run the master and node processes on that VM and run a simple hello world example. Here we won't be using <code>minikube</code>, but <code>kubeadm</code> to be more closer to a multi-node setup.</p> <p>The default behavior of a kubelet is to fail to start if swap memory is detected on a node. So it is recommended to disable swap:</p> <pre><code>sudo swapoff -a\nsudo sed -i '/ swap / s/^/#/' /etc/fstab\n</code></pre> <p>Disable SELinux (Security-Enhanced Linux) to allow containers to access the host filesystem; for example, some cluster network plugins require that.</p> <pre><code># Set SELinux in permissive mode (effectively disabling it; violations logged)\nsudo setenforce 0\nsudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\n</code></pre> <pre><code>sudo dnf update -y\n</code></pre> <p>Update repos. Exclude is recommended to avoid these packages being updated with <code>dns update</code>.</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://pkgs.k8s.io/core:/stable:/v1.34/rpm/\nenabled=1\ngpgcheck=1\ngpgkey=https://pkgs.k8s.io/core:/stable:/v1.34/rpm/repodata/repomd.xml.key\nexclude=kubelet kubeadm kubectl cri-tools kubernetes-cni\nEOF\n</code></pre> <ul> <li>kubeadm: the command to bootstrap the cluster.</li> <li>kubelet: the component that runs on all of the machines in the cluster and does things like starting pods and containers.</li> <li>kubectl: the command line util to talk to the cluster.</li> </ul> <pre><code>sudo dnf install kubeadm kubelet kubectl --disableexcludes=kubernetes -y\n</code></pre> <p>Check containerd is running:</p> <pre><code>sudo systemctl status containerd --no-pager\n</code></pre> <p>Start <code>kubeadm</code>:</p> <pre><code>sudo kubeadm init \\\n  --apiserver-advertise-address=$(hostname -I | awk '{print $1}') \\\n  --pod-network-cidr=10.244.0.0/16\n</code></pre> <p>This command sets up the control plane and writes the admin\u2019s kubeconfig file here (owned by root): <code>/etc/kubernetes/admin.conf</code> This file tells <code>kubectl</code>:</p> <ul> <li>where to find the API server,</li> <li>which user credentials to use,</li> <li>what certificates to trust.</li> </ul> <p>To start the cluster:</p> <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> <p>If you run the following command, it will show the control-plane node is not ready as it requires a pod network plugin---Kubernetes depends on a CNI (Container Network Interface) plugin for inter-pod communication.</p> <pre><code>kubectl get nodes\n</code></pre> <pre><code>kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml\nkubectl -n kube-flannel get pods -o wide\n</code></pre> <p>After this is done the <code>kubectl get nodes</code> will show the control plane node as ready.</p> <pre><code>kubectl get nodes\nNAME          STATUS   ROLES           AGE   VERSION\nvmnetto3160   Ready    control-plane   42m   v1.34.1\n</code></pre> <p>One can also see all pods running:</p> <pre><code>kubectl get pods -A\nNAMESPACE      NAME                                  READY   STATUS    RESTARTS   AGE\nkube-flannel   kube-flannel-ds-hd7gw                 1/1     Running   0          11m\nkube-system    coredns-66bc5c9577-s2vpb              1/1     Running   0          52m\nkube-system    coredns-66bc5c9577-vnpb8              1/1     Running   0          52m\nkube-system    etcd-vmnetto3160                      1/1     Running   0          52m\nkube-system    kube-apiserver-vmnetto3160            1/1     Running   0          52m\nkube-system    kube-controller-manager-vmnetto3160   1/1     Running   0          52m\nkube-system    kube-proxy-v7kc6                      1/1     Running   0          52m\nkube-system    kube-scheduler-vmnetto3160            1/1     Running   0          52m\n</code></pre> <p>As we are running everything on a single VM, the following command would not be used; as it is for additional VMs. The <code>kubeadm init</code> command already starts <code>kubelet</code> on the node.</p> <pre><code>sudo kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;\n</code></pre> <p>To allow regular nodes to be scheduled in control-plane node (okay for single node tests).</p> <pre><code># see the taint there in control-plane\nkubectl describe node vmnetto3160 | grep Taints\n\n# remove it\nkubectl taint nodes --all node-role.kubernetes.io/control-plane-\n</code></pre> <p>Deplay a pod:</p> <pre><code>kubectl run nginx --image=nginx --restart=Never\n</code></pre>"},{"location":"containers/k8s/#example-k8s-on-multi-node-cluster","title":"Example k8s on multi-node cluster","text":"<p>Assuming there is a master node running (with all the steps above) and a new almalinux VM is available; here is what it takes to have that second VM join the k8s cluster as a worker node.</p> <ul> <li>Disable swap (see above)</li> <li>Install kubernetes package (see above)</li> </ul> <p>The use the join command (replace xxx with actual token/cert): <pre><code>kubeadm join 10.31.0.4:6443 --token xxxxxx.xxxxxxxxxxxxxxxx \\\n&gt;         --discovery-token-ca-cert-hash sha256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n</code></pre></p> <p>On the master node one should see the new worker node:</p> <pre><code>kubectl get nodes -o wide\n</code></pre>"},{"location":"containers/k8s/#test-multi-node","title":"Test multi-node","text":"<p>Or create a DaemonSet. Create a file:</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-ip-printer\nspec:\n  selector:\n    matchLabels:\n      app: node-ip-printer\n  template:\n    metadata:\n      labels:\n        app: node-ip-printer\n    spec:\n      containers:\n      - name: printer\n        image: busybox\n        command:\n          - sh\n          - -c\n          - |\n            echo \"Node: $(hostname) Host IP: $NODE_IP\"\n            sleep 3600\n        env:\n        - name: NODE_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n      restartPolicy: Always\n</code></pre> <p>Run:</p> <pre><code>kubectl apply -f node-ip-printer.yaml\n</code></pre> <p>Check log <pre><code>kubectl get pods -o wide\nkubectl logs &lt;pod-name&gt;\n</code></pre></p> <p>Get more info about the pod</p> <pre><code>kubectl describe pod &lt;pod-name&gt;\n</code></pre> <p>To delete the pod</p> <pre><code>kubectl delete -f node-ip-printer.yaml\n</code></pre> <p>Or</p> <pre><code>kubectl delete pod &lt;pod-name&gt;\n</code></pre> <p>Get inside a pod</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -- sh\n</code></pre>"},{"location":"containers/k8s/#appendix","title":"Appendix","text":"<p>Check <code>kubelet</code> logs:</p> <pre><code>sudo journalctl -u kubelet -n 20 --no-pager\n</code></pre> <p>If you lose the command to join the worker (by creating a new token):</p> <pre><code>kubeadm token create --print-join-command\n</code></pre> <p>To use existing one:</p> <pre><code># get token\nsudo kubeadm token list\n\n# get discovery-token-ca-cert-hash\nopenssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | \\\nopenssl rsa -pubin -outform der 2&gt;/dev/null | \\\nopenssl dgst -sha256 -hex | \\\nsed 's/^.* //'\n</code></pre> <p>See system pods: <code>kubectl get pods -n kube-system</code></p> <p>See namespaces: <code>kubectl get namespaces</code> or <code>kubectl get ns</code></p> <p>See metrics:</p> <pre><code># install metrics server\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\n# see metrics on nodes\nkubectl top nodes\n\n# see metrics on pods\nkubectl top pods -A\n</code></pre>"},{"location":"containers/k8s/#references","title":"References","text":"<ul> <li>https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</li> <li>https://kubernetes.io/docs/reference/networking/ports-and-protocols/</li> <li>tutorial (ubuntu install k8s 1.33): https://www.youtube.com/watch?v=j3a2Sr2n8eQ</li> </ul>"},{"location":"containers/minikubeintro/","title":"Minikube intro","text":"<p>Minikube is a tool that simplifies running a local Kubernetes cluster by launching a single-node cluster on a laptop/desktop/server using virtual machines, containers, or bare metal (depending on OS). It uses the same upstream Kubernetes binaries as production environments, making it ideal for development, testing, and learning Kubernetes without the overhead of managing a full-scale cluster. Minikube is not a separate version or subset of Kubernetes\u2014it doesn't modify Kubernetes core functionality or APIs. Instead, it is a setup and convenience layer around Kubernetes that streamlines installation, networking, and basic tooling for local use. It is not intended for production use, nor does it replicate the performance or complexity of multi-node, cloud-native Kubernetes clusters.</p>"},{"location":"containers/minikubeintro/#install-minikube-macos","title":"Install minikube (macos)","text":"<pre><code>brew install minikube\nbrew install kubectl\n</code></pre>"},{"location":"containers/minikubeintro/#start-minikube","title":"Start minikube","text":"<pre><code>minikube start --driver=docker\n</code></pre> <p>This launches a local Kubernetes cluster (in a docker container). This will download the VM image too in case it does not exist.</p>"},{"location":"containers/minikubeintro/#interact-with-the-cluster","title":"Interact with the cluster","text":"<p>Start minikube dashboard</p> <pre><code>minikube dashboard\n</code></pre> <p>See all pods in all namespaces (<code>get po</code> short for <code>get pods</code> and <code>-A</code> short for <code>--all-namespaces</code>).</p> <pre><code>$ kubectl get po -A\nNAMESPACE              NAME                                         READY   STATUS    RESTARTS        AGE\nkube-system            coredns-674b8bbfcf-sgpsl                     1/1     Running   0               5m16s\nkube-system            etcd-minikube                                1/1     Running   0               5m23s\nkube-system            kube-apiserver-minikube                      1/1     Running   0               5m22s\nkube-system            kube-controller-manager-minikube             1/1     Running   0               5m22s\nkube-system            kube-proxy-n55xw                             1/1     Running   0               5m17s\nkube-system            kube-scheduler-minikube                      1/1     Running   0               5m23s\nkube-system            storage-provisioner                          1/1     Running   1 (4m46s ago)   5m20s\nkubernetes-dashboard   dashboard-metrics-scraper-5d59dccf9b-724xl   1/1     Running   0               3m5s\nkubernetes-dashboard   kubernetes-dashboard-7779f9b69b-7mvvp        1/1     Running   0               3m5s\n</code></pre> <p></p>"},{"location":"containers/minikubeintro/#quick-description-of-the-pods","title":"Quick description of the pods","text":"<ul> <li> <p>coredns: Provides DNS resolution inside the cluster so services can find and communicate with each other.</p> </li> <li> <p>etcd: A distributed key-value store used to hold all cluster data and configuration.</p> </li> <li> <p>kube-apiserver: Serves as the front end of the Kubernetes control plane, exposing the Kubernetes API.</p> </li> <li> <p>kube-controller-manager: Runs controller processes that handle routine cluster tasks such as replication and node lifecycle management.</p> </li> <li> <p>kube-proxy: Maintains network rules on nodes, enabling network communication between pods and services.</p> </li> <li> <p>kube-scheduler: Assigns newly created pods to nodes based on resource availability and constraints.</p> </li> <li> <p>storage-provisioner: Automatically provisions persistent volumes for pods based on PersistentVolumeClaims (PVCs).</p> </li> <li> <p>dashboard-metrics-scraper: Collects resource usage metrics (CPU, memory) for display in the Kubernetes Dashboard.</p> </li> <li> <p>kubernetes-dashboard: A web-based UI for managing and monitoring Kubernetes resources.</p> </li> </ul>"},{"location":"containers/minikubeintro/#deploy-a-hello-world-app","title":"Deploy a hello world app","text":"<p>Create and expose deployment.</p> <pre><code>kubectl create deployment hello-minikube --image=kicbase/echo-server:1.0\nkubectl expose deployment hello-minikube --type=NodePort --port=8080\n</code></pre> <p>To see the service is running:</p> <pre><code>kubectl get services hello-minikube\n</code></pre> <p>To access the service by allowing minikube to launch a webserver:</p> <pre><code>minikube service hello-minikube\n</code></pre> <p>In macos you may get this error (in case you are using qemu driver):</p> <pre><code>Exiting due to MK_UNIMPLEMENTED: minikube service is not currently implemented with the builtin network on QEMU, try starting minikube with '--network=socket_vmnet'\n</code></pre> <p>One way to overcome this is:</p> <pre><code>kubectl port-forward service/hello-minikube 7080:8080\n</code></pre> <p>Then open the browser: <code>http://localhost:7080/</code></p> <p>Another is to install <code>socket_vmnet</code> (see instructions), then start the process to create minikube again, as it will detect the <code>socket_vmnet</code> installation.</p>"},{"location":"containers/minikubeintro/#delete-cluster","title":"Delete cluster","text":"<p>This will stop the VM running minikube and delete the cluster state.</p> <pre><code>minikube delete\n</code></pre>"},{"location":"containers/minikubeintro/#references","title":"References","text":"<ul> <li>Minikube</li> <li>Kubernetes 101</li> <li>Minikube macos tutorial</li> </ul>"},{"location":"hpc/","title":"HPC","text":"<p>Links, codes, tutorials on High Performance Computing (HPC).</p>"},{"location":"hpc/azurehpc/","title":"Azure adventures","text":"<p>Set of tutorials to work with Azure cloud/hpc technologies.</p> <p>Website: Azure Adventures</p>"},{"location":"hpc/hpcadvisor/","title":"HPC Advisor","text":"<p>Tool is to assist users in selecting a cluster configuration based on actual application executions. Underneath, the execution of the application is based on resources provisioned by Azure Batch; but the data collected can be used to make decisions for other environments including Azure CycleCloud, Azure VMSS, Azure VMs, AKS, etc.</p> <p>URL: HPCAdvisor</p>"},{"location":"hpc/hpcapps/","title":"HPC Apps","text":"<p>Repo containing installation and example of executions for some HPC applications including WRF, GROMACS, OpenFOAM, NAMD, and LAMMPS.</p> <p>URL: HPCApps repo</p>"},{"location":"hpc/mpitestcodes/","title":"Simple MPI test codes","text":"<p>Just some simple MPI code for tests.</p> <p>Source folder on GitHub: folder</p>"},{"location":"misc/ansible/","title":"Ansible intro","text":"<p>Ansible is an open-source automation tool for configuration management, application deployment, and orchestration. It uses YAML playbooks to describe the desired state of systems and communicates over SSH. It is agentless, which means there is no need to install anything on the target machines.</p>"},{"location":"misc/ansible/#why-ansible","title":"Why Ansible?","text":"<ul> <li>Agentless: No need to install extra software on managed machines;</li> <li>Idempotent: Tasks can be run repeatedly without unwanted side effects;</li> <li>Human-readable: Playbooks use YAML, easy to write and review;</li> <li>Scalable: Manage a handful of servers or thousands with the same commands.</li> </ul> <p>Some things we have inside a Dockerfile for instance are specific for docker. However, if you put the instructions in an ansible playbook, those can be executed outside docker, making this solution more reusable by different environments. If you are working with docker you could have also a playbook to have instructions that are targetted to the host and to the docker container.</p>"},{"location":"misc/ansible/#alternatives","title":"Alternatives","text":"<p>If Ansible is not available, one could use:</p> <ul> <li>Shell scripts with <code>ssh</code> and <code>scp</code> for manual automation;</li> <li>Configuration management tools like Puppet or  Chef (these often require agents on target mmachines);</li> <li>Cloud-native tools such as AWS CloudFormation, Terraform, or Azure Resource Manager (for infrastructure provisioning).</li> </ul>"},{"location":"misc/ansible/#major-components","title":"Major components","text":"<ul> <li>Inventory: A list of machines (hosts) Ansible manages.</li> <li>Modules: Small, reusable programs that perform one action (install a package, copy a file, manage a service, etc.).</li> <li>Tasks: Single steps executed on target hosts; each task calls a module with parameters.</li> <li>Plays: A mapping that says \u201crun these tasks on these hosts\u201d.</li> <li>Playbooks: YAML files containing one or more plays. They describe how and which order, at what time and where, and what modules should be executed. It orchestrates the modules' execution.</li> <li>Handlers: Special tasks executed only when notified by other tasks (useful for actions like restarting a service after config change).</li> <li>Roles: Redistributable units of organization that allow users to share automation code easier.</li> <li>Variables: Key\u2013value pairs to parametrize playbooks, templates, and roles.</li> </ul>"},{"location":"misc/ansible/#simple-test-vm","title":"Simple test VM","text":"<p>Simple example, for instance, once you provision a cloud VM (e.g. almalinux vm).</p> <pre><code>sudo dnf -y install epel-release\nsudo dnf -y install ansible\nansible --version\n</code></pre> <p>Create inventory file (<code>inventory</code>):</p> <pre><code>[local]\nlocalhost ansible_connection=local\n</code></pre> <p>Create playbook file (<code>simple.yml</code>). This is a playbook with a single play and three tasks.</p> Simple<pre><code>- name: Simple Ansible test on AlmaLinux\n  hosts: local\n  connection: local\n  tasks:\n    - name: Gather system facts\n      setup:\n    - name: Create a test file\n      copy:\n        dest: /tmp/ansible_hello.txt\n        content: \"Hello from Ansible at {{ ansible_date_time.iso8601 }}\\n\"\n    - name: Show hostname\n      debug:\n        msg: \"This VM's hostname is {{ ansible_hostname }}\"\n</code></pre> <p>Run the line below and the file in <code>/tmp/</code> should be created.</p> <pre><code>ansible-playbook -i inventory simple.yml\n</code></pre> <p>If one wants to use multiple machines, replace 'hosts: local' with 'hosts: all' in the <code>simple.yml</code> and update the inventory with the list of ip addresses. Also remove <code>connection: local</code>.</p>"},{"location":"misc/ansible/#further-notes","title":"Further notes","text":"<p>List available modules:</p> <pre><code>ansible-doc -l\n</code></pre> <p>More details on a particular module (e.g. <code>copy</code>):</p> <pre><code>ansible-doc copy\n</code></pre> <p>Can also find modules docs online at: WEBSITE</p> <p>Test connections:</p> <pre><code>ansible all -m ping -i inventory\n</code></pre> <p>Show inventory:</p> <pre><code>ansible-inventory --list -y -i inventory\n</code></pre> <p>Append extra verbose mode using <code>-vvv</code>:</p> <pre><code>ansible-playbook -i inventory simple.yml -vvv\n</code></pre>"},{"location":"misc/blender/","title":"Blender","text":"<p>Blender 4.5 (video list): overview for beginners (UI, shortcuts, object transformation, animation, texture, light, etc..) HERE</p> <p>Simple: \"move the cube\" HERE</p> <p>Simple: \"move camera\" HERE</p> <p>Simple: \"move camera\" HERE</p> <p>Simple: \"move camera around\" HERE</p>"},{"location":"misc/grafana/","title":"Grafana","text":"<p>Grafana is a data visualization and monitoring tool. It connects to various data sources (like Prometheus, InfluxDB, MySQL, etc.) and allows one to create dashboards, graphs, and alerts to monitor metrics in real time.</p> <p>Here is a quick example with grafana + prometheus + node exporter on localhost (macos).</p> Component Port Purpose Node Exporter 9100 Exposes system metrics (CPU, memory, disk, etc.) Prometheus 9090 Collects metrics and stores them Grafana 3000 Visualizes metrics in dashboards <p>Assuming Node Exporter and Prometheus are already running.</p> <pre><code>brew install grafana\nbrew services start grafana\n</code></pre> <p>Open browser(admin/admin): <code>http://localhost:3000/login</code></p> <p>Go to Settings \u2192 Data Sources \u2192 Add data source \u2192 Prometheus.  Then add <code>http://localhost:9090</code>.</p> <p>To add node exporter; go to Dashboards, then import the dashboard using this ID: <code>1860</code>. This will import a pre-built dashboard for node exporter.</p> <p>To stress cpu and see changes in the dashboard:</p> <pre><code>brew install stress\nstress --cpu 4 --timeout 60\n</code></pre> <p>Alternatively, one can import dashboards:</p> cpu<pre><code>{\n  \"id\": null,\n  \"title\": \"CPU Utilization\",\n  \"tags\": [\n    \"example\"\n  ],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"5s\",\n  \"panels\": [\n    {\n      \"type\": \"timeseries\",\n      \"title\": \"CPU Utilization (%)\",\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"expr\": \"100 * (1 - avg(rate(node_cpu_seconds_total{mode=\\\"idle\\\"}[2m])))\",\n          \"legendFormat\": \"CPU Utilization\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"x\": 0,\n        \"y\": 0,\n        \"w\": 24,\n        \"h\": 9\n      }\n    }\n  ]\n}\n</code></pre> cpuanddisk<pre><code>{\n  \"id\": null,\n  \"title\": \"CPU and Disk Usage\",\n  \"tags\": [\n    \"example\"\n  ],\n  \"timezone\": \"browser\",\n  \"schemaVersion\": 39,\n  \"version\": 1,\n  \"refresh\": \"5s\",\n  \"panels\": [\n    {\n      \"type\": \"timeseries\",\n      \"title\": \"CPU Utilization (%)\",\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"expr\": \"100 * (1 - avg(rate(node_cpu_seconds_total{mode=\\\"idle\\\"}[2m])))\",\n          \"legendFormat\": \"CPU Utilization\",\n          \"refId\": \"A\"\n        }\n      ],\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"x\": 0,\n        \"y\": 0,\n        \"w\": 24,\n        \"h\": 9\n      }\n    },\n    {\n      \"type\": \"timeseries\",\n      \"title\": \"Disk Usage (%)\",\n      \"datasource\": {\n        \"type\": \"prometheus\",\n        \"uid\": \"prometheus\"\n      },\n      \"targets\": [\n        {\n          \"expr\": \"100 - (100 * (node_filesystem_avail_bytes{fstype!=\\\"tmpfs\\\",mountpoint!=\\\"/boot\\\"} / node_filesystem_size_bytes{fstype!=\\\"tmpfs\\\",mountpoint!=\\\"/boot\\\"}))\",\n          \"legendFormat\": \"{{mountpoint}}\",\n          \"refId\": \"B\"\n        }\n      ],\n      \"fieldConfig\": {\n        \"defaults\": {\n          \"unit\": \"percent\",\n          \"min\": 0,\n          \"max\": 100\n        },\n        \"overrides\": []\n      },\n      \"gridPos\": {\n        \"x\": 0,\n        \"y\": 9,\n        \"w\": 24,\n        \"h\": 9\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"misc/mlperf/","title":"MLPerf","text":"<p>MLPerf is an open, industry-standard benchmark suite for measuring machine learning performance. It\u2019s maintained by the MLCommons consortium (which includes NVIDIA, Google, Intel, and others). MLPerf is primarily categorized by two major types of ML processes: training and inference. There are also specialized benchmarks for specific hardware and workloads. Original documents describing MLPerf can be found here: MLPerf Training and MLPerf Inference. Those are interesting to understand some of the key motivations behind these benchmarks, including the uniqueness of ML/DL workloads w.r.t. benchmarking.</p> <p>MLPerf evaluates systems on standardized ML workloads across multiple domains, including:</p> <ul> <li>Image classification (e.g., ResNet-50)</li> <li>Object detection (e.g., SSD, Mask R-CNN)</li> <li>Language modeling (e.g., BERT)</li> <li>Recommendation systems (e.g., DLRM)</li> <li>Speech recognition (e.g., RNN-T)</li> </ul> <p>Full list for training can be found here and inference can be found here.</p> <p>One can use MLPerf results to: (i) Compare hardware (e.g., GPU vs CPU); (ii) Tune software stacks (CUDA, PyTorch, TensorFlow); and (iii) Validate scaling behavior or deployment efficiency.</p>"},{"location":"misc/mlperf/#testing-on-a-single-vm","title":"Testing on a single VM","text":""},{"location":"misc/mlperf/#training","title":"Training","text":"<p>Example: Single Stage Detector.</p> <p>In this example we assume we have a VM in azure with SKU <code>Standard_NC40ads_H100_v5</code> and image <code>almalinux:almalinux-hpc:8_10-hpc-gen2:latest</code>. Once machine is provisioned, check if gpus+cuda is detected via: <code>nvidia-smi</code>.</p> prepmlperf<pre><code>#!/bin/bash\n# script to do all setup, data download, and training\n# based on https://github.com/mlcommons/training/tree/master/single_stage_detector\n\nexport DATADIR=\"/datadrive\"\nexport MYDATA=\"$DATADIR/mydata\"\nsudo groupadd docker\nsudo usermod -aG docker $USER\nnewgrp docker\ndocker run hello-world\n\nsudo jq '. + {\"data-root\": \"/mnt/docker\"}' /etc/docker/daemon.json | sudo tee /etc/docker/daemon.json.tmp &gt;/dev/null &amp;&amp; sudo mv /etc/docker/daemon.json.tmp /etc/docker/daemon.json &amp;&amp; sudo systemctl restart docker\ndocker info | grep 'Docker Root Dir'\n\ncd $DATADIR\ngit clone https://github.com/mlcommons/training.git\n\n# alternative for faster install (with possible side effects;e.g install on other python versions)\n# pip3 install --prefer-binary opencv-python-headless\n# sed -i.bak -E 's/load_zoo_dataset\\(\\s*name=\"([^\"]+)\"\\s*,/load_zoo_dataset(\"\\1\",/' fiftyone_openimages.py\n\npip3 install fiftyone\npython -m pip show fiftyone\nsudo ln -s $(which python3) /usr/local/bin/python\ncd $DATADIR/training/single_stage_detector/scripts\n\necho \"Downloading dataset... this will take several minutes\"\n./download_openimages_mlperf.sh -d $MYDATA\n\n# prepare docker\ncd $DATADIR/training/single_stage_detector/\ndocker build -t mlperf/single_stage_detector .\ndocker run --rm -it --gpus=all --ipc=host -v $MYDATA:/datasets/open-images-v6-mlperf mlperf/single_stage_detector bash\n\n# inside the container:\n# apt-get update ; apt-get install vim -y\n# sed -i '0,/\\${SLURM_LOCALID-}/s//${SLURM_LOCALID:-0}/' run_and_time.sh\n# update config_DGXA100_001x08x032.sh\n# DGXNGPU=1, DGXSOCKETCORES=20, DGXNSOCKET=1, DGXHT=2\n# source config_DGXA100_001x08x032.sh\n# -----------------------------------------------------------\n# conda create -n torch212 python=3.10\n# conda init bash\n# source ~/.bashrc\n# conda activate torch212\n# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n#\n# fix: coco_eval.py\n# # Old line:\n# import torch._six\n# #Replace with\n# import collections.abc as container_abcs\n#\n# pip3 install mlperf_logging\n# pip3 install pycocotools\n# ./run_and_time.sh\n</code></pre> <p>Once run script is started, output should be:</p> <pre><code>Epoch: [0]  [    0/36571]  eta: 1 day, 20:19:44  lr: 0.000000  loss: 2.2699 (2.2699)  classification: 1.5590 (1.5590)  bbox_reg\nression: 0.7109 (0.7109)  time: 4.3637  data: 2.2989  max mem: 51676\nEpoch: [0]  [   20/36571]  eta: 6:17:02  lr: 0.000000  loss: 2.1944 (2.2521)  classification: 1.4886 (1.5371)  bbox_regression:\n 0.7036 (0.7150)  time: 0.4317  data: 0.0003  max mem: 52125\nEpoch: [0]  [   40/36571]  eta: 5:20:59  lr: 0.000000  loss: 2.1934 (2.2440)  classification: 1.4949 (1.5292)  bbox_regression:\n 0.6956 (0.7148)  time: 0.4309  data: 0.0003  max mem: 52125\nEpoch: [0]  [   60/36571]  eta: 5:03:05  lr: 0.000000  loss: 2.2322 (2.2630)  classification: 1.5102 (1.5478)  bbox_regression:\n 0.7024 (0.7151)  time: 0.4384  data: 0.0004  max mem: 52125\n.\n.\n.\n</code></pre>"},{"location":"misc/mlperf/#multiple-vms","title":"Multiple VMs","text":"<p>Similar to single VM but a cluster needs to be provision and parameters related to SLURM need to be adjusted. See details HERE.</p>"},{"location":"misc/prometheus/","title":"Prometheus","text":"<p>Prometheus is an open-source monitoring and alerting system. It collects metrics from applications and systems, stores them in a time-series database, and lets one query or visualize the data easily.</p>"},{"location":"misc/prometheus/#test-locally-macos","title":"Test locally (macos)","text":"<pre><code>brew install prometheus\nbrew install node_exporter\n</code></pre> <p>Run node exporter and test it:</p> <pre><code>node_exporter\ncurl http://localhost:9100/metrics\n</code></pre> <p>Create <code>prometheus.yml</code>:</p> <pre><code>global:\n  scrape_interval: 5s  # how often to collect metrics\n\nscrape_configs:\n  - job_name: \"node\"\n    static_configs:\n      - targets: [\"localhost:9100\"]\n</code></pre> <p>Run prometheus:</p> <pre><code>prometheus --config.file=prometheus.yml\n</code></pre> <p>Open browser: <code>localhost:9090/query</code></p> <p>In the expression enter this to see the overal system utilization)</p> <pre><code>100 * (1 - avg(rate(node_cpu_seconds_total{mode=\"idle\"}[2m])))\n</code></pre> <p>One can also go to the settings menu to change to local time zone, as default is UTC.</p>"},{"location":"misc/prometheus/#refereces","title":"Refereces","text":"<ul> <li>https://prometheus.io/docs/prometheus/latest/getting_started/</li> </ul>"}]}